# MC_WS17-18_2.PNG

**NOte**: a)-c) are multiple choice. d)-f) are questions to be answered with plain text.

a) Q: What is the main advantage of having a convex loss function?

A: They are relatively easy to optimize.
**Note**: Because there exists only one minimum which is per definition the global minimum.

b) Q: The k-means algorithm is an example of which of the following?

A: Unsupervised learning
**Note**: k-means is a clustering algorithm

c) Q: he gradient of a function f points in the direction of ____ of f.

A: steepest ascent


d) Q: What is the aim of supervised machine learning?

A: To train a classifier f with/on a given set of inputs and their respective labels (values in Regression) in order to accurately predict the labels of yet unseen datapoints.


e) Q: Describe the phenomena of “overfitting” and give an example of a technique one can employ to avoid it.

A: In overfitting, a too complex classifier  that fits the training(!) data too well and that does therefore not accurately predict yet unseen data. The classifier does not "generalize" well to new data.


f) Q: Describe how the random forest algorithm works. You may assume that it is known how decision trees work.

A: From the training data select a random sample with replacements (note, if the original dataset has n datapoints, the sample has n datapoins as well).
(This, with the below described averaging/majority-vote is called bagging.)
Grow a decision tree on the dataset, but to determine each split of internal nodes a random subset of features (m<d) is selected and the optimal split is determined among those m features.
(This can be called feature-bagging and distinguishes the random forest from just plain bagging.)
Using the above procedure, arbitrarily many (users choice) decision trees are built which form the random forest.
The prediction of the random forest is then done using e.g. majority-vote for classification or averaging for regression.
