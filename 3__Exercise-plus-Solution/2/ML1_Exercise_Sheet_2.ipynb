{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning I, Programming Exercise 2\n",
    "\n",
    "## 1. SVM Basics\n",
    "Before we actually attempt to implement a support vector machine algorithm ourselves, it makes sense to play around with the reference implementation from Scikit-Learn. That way, we get to know what kind of behaviour we can and should expect from a working implementation.\n",
    "\n",
    "1. Let $\\{x \\in \\mathbb{R}^2 \\mid \\mathbf{w}^T\\mathbf{x} + b = 0\\}$ be a hyperplane (i.e., a line). Write a function that takes arguments $\\mathbf{x}^{(1)}_1, \\mathbf{x}^{(2)}_1, \\ldots, \\mathbf{x}^{(n)}_1 \\in \\mathbb{R}$, $\\mathbf{w} \\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$ and returns $\\mathbf{x}^{(1)}_2, \\mathbf{x}^{(2)}_2, \\ldots, \\mathbf{x}^{(n)}_2 \\in \\mathbb{R}$ s.t.\n",
    "\\begin{equation*}\n",
    "    \\mathbf{w}^T \\left(\\begin{matrix} \\mathbf{x}^{(i)}_1 \\\\ \\mathbf{x}^{(i)}_2 \\end{matrix}\\right) + b = 0.\n",
    "\\end{equation*}\n",
    "You can use this function to easily draw line segments from the line defined by $\\mathbf{w}$ and $b$.\n",
    "2. Train an SVM (Scikit-Learn's [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class) on both datasets generated in the code skeleton. Choose `kernel='linear'` in order to get a linear classifier.\n",
    "3. Draw the decision surface of the SVM. After you have trained the SVM you can access $\\mathbf{w}$ and $b$ via `SVC.coef_` and `SVC.intercept_`, respectively.\n",
    "4. Play around with the value of $C$ and see how that changes the decsion surface.\n",
    "5. Implement a function that computes the signed distance between a set of points $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots$ and the hyperplane defined by $\\mathbf{w}$ and $b$. Use this to compute the signed distance between all datapoints and the decision surface of the SVM and multiply the result with the class label ($\\{-1, 1\\}$) of the datapoint. The points with minimal but non-negative value are the support vectors.\n",
    "6. Based on the support vector(s), draw the margin of the SVM as two lines parallel to the decision surface. One of them must contain the support vector, while the other line needs to be on the opposite side of the decision boundary.\n",
    "7. Pick two or three datapoints from each class and flip their label. How does the SVM react to those outliers? Train it again on this dataset and observe what happens for different values of $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def apply_line(x, w, b):\n",
    "    raise NotImplementedException('TODO') \n",
    "\n",
    "def signed_distance(x, w, b):\n",
    "    raise NotImplementedException('TODO') \n",
    "\n",
    "\n",
    "# Generate toy data\n",
    "n = 200\n",
    "X_moon, Y_moon = datasets.make_moons(n, noise=0.2, random_state=123456)\n",
    "X_blob, Y_blob = datasets.make_classification(n, n_features=2, n_redundant=0, \n",
    "                                              n_clusters_per_class=1, class_sep=1.2, random_state=12345)\n",
    "# Change labels from 0,1 to -1,1\n",
    "Y_moon *= 2; Y_moon -= 1\n",
    "Y_blob *= 2; Y_blob -= 1\n",
    "\n",
    "\n",
    "# Plot the datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "cmap = ListedColormap([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "for (name, X, Y), ax in zip([('Two Moons', X_moon, Y_moon), ('Blobs', X_blob, Y_blob)], axes):\n",
    "    # Plot training Data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=12345)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap, edgecolor='k')\n",
    "    ax.grid(linestyle='dashed')\n",
    "    ax.set_autoscale_on(False)\n",
    "    \n",
    "    # TODO: Train and visualize the SVM\n",
    "    \n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Nearest Centroid Classifier\n",
    "Given a dataset $D = \\{(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\}$ with labels $y \\in \\{-, +\\}$, we can separate the dataset per class, i.e., $D_- = \\{(\\mathbf{x}, y) \\in D \\mid y=-\\}$ and $D_+ = \\{(\\mathbf{x}, y) \\in D \\mid y=+\\}$ and compute the centroids for each class:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{c}_- = \\frac{1}{\\lvert D_- \\rvert} \\sum_{(\\mathbf{x}, y) \\in D_-} \\mathbf{x} \\qquad\\qquad \\mathbf{c}_+ = \\frac{1}{\\lvert D_+ \\rvert} \\sum_{(\\mathbf{x}, y) \\in D_+} \\mathbf{x}.\n",
    "\\end{equation*}\n",
    "The nearest centroid classifier $f$ is then defined as follows:\n",
    "\\begin{equation*}\n",
    "    f(\\mathbf{x}) := \\operatorname{arg\\ min}_{y \\in \\{-,+\\}} \\lVert \\mathbf{x} - \\mathbf{c}_y \\rVert\n",
    "\\end{equation*}\n",
    "\n",
    "In this exercises we will implement the Nearest Centroid Classifier (NCC) and run it on two generated toy datasets. Please keep in mind that Scikit-learn returns labels in $\\{0,1\\}$, which might become a problem depending on the way you decide to implement the NCC.\n",
    "1. Write a function that takes a dataset $D$ and returns the centroids $\\mathbf{c}_-$ and $\\mathbf{c}_+$.\n",
    "2. Then use these centroids to implement the nearest centroid classifier.\n",
    "3. Draw the decision surface defined by $f(\\mathbf{x}) = 0$ into the corresponding scatterplot that is already created by the code skeleton. Also plot and highlight the centroids and draw a line between them.\n",
    "4. Compute the accuracy of the NCC on the held-out test dataset. Can you find a way to improve it without actuallly modifying the NCC algorithm itself? (Hint: Take a look at part 3 of the last notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "def compute_centroids(X, y):\n",
    "    raise NotImplementedException('TODO')       \n",
    "\n",
    "class NCC(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        super(NCC, self).__init__()\n",
    "        raise NotImplementedError('TODO')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "    \n",
    "# Generate toy data\n",
    "n = 200\n",
    "X_moon, Y_moon = datasets.make_moons(n, noise=0.2, random_state=123456)\n",
    "X_blob, Y_blob = datasets.make_classification(n, n_features=2, n_redundant=0, \n",
    "                                              n_clusters_per_class=1, random_state=234567)\n",
    "\n",
    "# Plot the datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "cmap = ListedColormap([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "for (name, X, Y), ax in zip([('Two Moons', X_moon, Y_moon), ('Blobs', X_blob, Y_blob)], axes):\n",
    "    # Plot dataset\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=cmap, edgecolor='k')\n",
    "    ax.grid(linestyle='dashed')\n",
    "    x_min, x_max, _, _ = ax.axis('equal')\n",
    "    \n",
    "    # TODO: Run and Visualize the NCC\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=12345)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
