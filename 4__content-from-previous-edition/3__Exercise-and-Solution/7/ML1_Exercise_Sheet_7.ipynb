{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning I, Programming Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Randomness & SGD\n",
    "As you have seen in the lecture, we can instantiate most machine learning models as special cases of a *generalized regression* problem by choosing a regularizer, a feature map and a loss function. For example, linear SVMs and a deep neural networks both fit into this general framework.\n",
    "\n",
    "In fact, this theoretical observation carries over (at least partly) into the real world, which we will show by implementing the linear SVM in PyTorch:\n",
    "1. Re-use your existing code from programming exercise 5.1 to implement a linear SVM in PyTorch. This is just a matter of finding the correct \"network\" architecture, loss function and regularizer. For your convenience, here is the soft-margin SVM optimization problem again:\n",
    "\\begin{equation*}\n",
    "    \\min_{\\mathbf{w} \\in \\mathbb{R}^d, b \\in \\mathbb{R}} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + \\frac{C}{n} \\sum_{i=1}^{n} \\max\\left(0, 1 - y^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)} + b)\\right)\n",
    "\\end{equation*}\n",
    "Note that the SVM should have only one output to distinguish between two classes, so you might need to adapt your training and prediction code slightly. However, make sure to keep the basic training routine intact:\n",
    "    * Shuffle the data\n",
    "    * Pass through all of the shuffled data in mini-batches and update the weights\n",
    "    * Repeat the previous steps for a given number of epochs\n",
    "\n",
    "Note that this procedure is slightly different from the linear SVM that we implemented in programming exercise 2: Instead of re-sampling a random batch with replacement before every update, we shuffle the data before each epoch and then process all of the data in batches *without* replacement. Technically, we are no longer using the textbook *stochastic gradient descent* to solve our optimization problem, but a method that is sometimes called *random reshuffling* in the optimization literature. We could also shuffle just once before training and then visit the data in the same order in every epoch, or we could not shuffle at all. While the latter two methods don't have a specific name for themselves, all three are special cases of a method usually called [*iterated gradient*](https://arxiv.org/pdf/1507.01030.pdf).\n",
    "\n",
    "Theoretically, these algorithms are indeed different and [it has been shown](https://arxiv.org/pdf/1510.08560.pdf) that, depending on the problem at hand, one algorithm might be strictly superior to another. But this is not a theory exercise, so we will try and see if we can find any empirical evidence showing the influence of the sampling strategy instead:\n",
    "2. Modify your training loop s.t. an additional option `shuffle` can be passed to the `TorchClassifier` class. Your implementation should support the four sampling startegies that we discussed above:\n",
    "    * `shuffle='never'`: Don't shuffle the data at all and just process it in the same, given order in every epoch.\n",
    "    * `shuffle='once'`: Shuffle the data once before the actual training loop and then visit the data in the same order in every subsequent epoch.\n",
    "    * `shuffle='epoch'`: Re-shuffle the data before each epoch.\n",
    "    * `shuffle='resample'`: Sample a batch from the training data with replacement before each gradient update.\n",
    "\n",
    "\n",
    "3. Choose a batch size of $B=20$, $C=2$ and use a step size of $0.01$ to train your SVM implementation on the given toy dataset for 15 epochs. You should train one SVM instance for each of the four algorithms from 2 and compare the results in a plot. Can you explain the results of your training?\n",
    "\n",
    "\n",
    "4. Compute and record the loss over the current batch of training data before each gradient update. To keep the curves from becoming too messy, you should keep a running mean of the values and reset it at the end of each epoch. After re-running this experiment for a few times, would you say that some of the algorithms converge faster than others on this specific problem?\n",
    "\n",
    "\n",
    "5. How do the results change if you use different values for $B$ and $C$? Feel free to experiment and then try to make sense of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LinearSVM(torch.nn.Module):\n",
    "    def __init__(self, in_features=2, bias=True):\n",
    "        super(LinearSVM, self).__init__()\n",
    "        \n",
    "        # TODO: instaniate layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "class SVMLoss(torch.nn.Module):\n",
    "    def __init__(self, model, C=1):\n",
    "        super(SVMLoss, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.C = C\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # TODO: Compute the SVM Loss as given in the exercise text\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "class TorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, torch_model, loss, optimizer_class, device, step_size=0.001, \n",
    "                 batch_size=64, epochs=20, shuffle='once'):\n",
    "        super(TorchClassifier, self).__init__()\n",
    "        \n",
    "        # TODO: Copy code from sheet 05\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO: Copy code from sheet 05\n",
    "        raise NotImplementedError('TODO')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: Copy code from sheet 05\n",
    "        raise NotImplementedError('TODO')\n",
    "    \n",
    "def plot_history(history):\n",
    "    # Plot training and validation curves for avg. loss and accuracy\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "    for (name, hist) in history:\n",
    "        data = [('Avg. Loss', hist['train_loss']), \n",
    "                ('Accuracy', hist['train_acc'])]\n",
    "        for ax, (metric, curve) in zip(axes, data):\n",
    "            ax.set_title(metric)\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "            ax.grid(linestyle='dashed')\n",
    "            ax.plot(range(1, len(curve)+1), curve, label=name)\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "# Generate toy data\n",
    "n = 2000\n",
    "n_classes = 2\n",
    "X, Y = make_classification(n, n_features=2, n_redundant=0, n_classes=n_classes, weights=(0.25, 0.75),\n",
    "                           flip_y=0, class_sep=1.3, shuffle=False, n_clusters_per_class=1, random_state=1234)\n",
    "Y *= 2; Y -= 1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=12345)\n",
    "# Sort the training data according to class labels. This is not a step you should ever do,\n",
    "# we just want to simulate a dataset that exists sorted on disk and is read in sequentially\n",
    "ind = np.argsort(Y_train)\n",
    "X_train = X_train[ind]\n",
    "Y_train = Y_train[ind]\n",
    "\n",
    "optimizer = torch.optim.SGD\n",
    "# Change this to 'cuda' if you can and want to use a GPU for training\n",
    "# Otherwise, you can completely ignore this parameter\n",
    "device = torch.device('cpu')  \n",
    "batch_size=20\n",
    "epochs = 15\n",
    "\n",
    "def get_classifier(C=1, step_size=0.001, shuffle='once'):\n",
    "    clf = LinearSVM(in_features = X.shape[1])\n",
    "    # TODO: make sure that the parameters in each classifier are inititalized to the same value\n",
    "    loss = SVMLoss(clf, C=C)\n",
    "    return TorchClassifier(clf, loss, optimizer, device, step_size=step_size, \n",
    "                           batch_size=batch_size, epochs=epochs, shuffle=shuffle)\n",
    "classifiers = [\n",
    "    ('SVM: C=0.25, no shuffling',\n",
    "     get_classifier(C=2, step_size=0.01, shuffle='never')),\n",
    "    ('SVM: C=0.25, shuffle once',\n",
    "     get_classifier(C=2, step_size=0.01, shuffle='once')),\n",
    "    ('SVM: C=0.25, shuffle before each epoch',\n",
    "     get_classifier(C=2, step_size=0.01, shuffle='epoch')),\n",
    "    ('SVM: C=0.25, sample before each update',\n",
    "     get_classifier(C=2, step_size=0.01, shuffle='resample'))\n",
    "]\n",
    "\n",
    "# Plot decision surface\n",
    "# First generate grid\n",
    "res = 200  # Resolution of the grid in cells\n",
    "x_max, y_max = np.max(X, axis=0)\n",
    "x_min, y_min = np.min(X, axis=0)\n",
    "x_min, x_max = x_min - (x_max - x_min) * 0.0625, x_max + (x_max - x_min) * 0.0625\n",
    "y_min, y_max = y_min - (y_max - y_min) * 0.0625, y_max + (y_max - y_min) * 0.0625\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(x_min, x_max, res),\n",
    "                             np.linspace(y_min, y_max, res))\n",
    "# Get test array from grid\n",
    "grid_input = np.c_[grid_x.reshape(-1), grid_y.reshape(-1)]\n",
    "\n",
    "cmap = colors.ListedColormap([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
    "rows = (len(classifiers)+1)//2\n",
    "history = []\n",
    "fig, axes = plt.subplots(rows, 2, sharex=True, sharey=True, figsize=(12, 4.5*rows))\n",
    "for (name, clf), ax in zip(classifiers, axes.ravel()):\n",
    "    #hist = clf.fit(X_train, Y_train)\n",
    "    #history.append((name, hist))\n",
    "    #score = clf.score(X_test, Y_test)\n",
    "    score = 1.0\n",
    "    #train_score = clf.score(X_train, Y_train)\n",
    "    train_score = 1.0\n",
    "    #grid_out = clf.predict(grid_input).reshape(grid_x.shape)\n",
    "\n",
    "    ax.set_title('%s,\\nTrain. Acc.: %.3f, Val. Acc.: %.3f' % (name, train_score, score))\n",
    "    #ax.contourf(grid_x, grid_y, grid_out, alpha=0.5, cmap=plt.cm.brg)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap, edgecolor='k')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# TODO: plot training loss\n",
    "#plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression with the LOOCV Trick\n",
    "In the lecture, you have seen that leave-one-out cross validation can be performed very efficiently for ridge regression. Indeed, the solution \n",
    "\\begin{equation*}\n",
    "    \\mathbf{w}_{\\text{RR}} := \\operatorname{arg\\ min}_{\\mathbf{w} \\in \\mathbb{R}^d} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + \\frac{C}{n} \\sum_{i=1}^{n} \\left(\\mathbf{w}^T\\mathbf{x}^{(i)} - y^{(i)}\\right)^2\n",
    "\\end{equation*}\n",
    "for ridge regression and also the root means squared error (RMSE) in LOOCV have a closed form and therefore it is not necessary to use an iterative optimization procedure. Note that this formulation also allows for a *bias*: We can simply add a constant feature to our data, effectively turning $w_{d+1}$ into a bias parameter.\n",
    "\n",
    "Your tasks will be the following:\n",
    "1. Implement the LOOCV trick as it is presented in the lecture and use this to find the best value for the regularization constant $C$ from a given set. Then, use this best value to find the solution $\\mathbf{w}_{\\text{RR}}$ over the whole training dataset. We will use [Boston Housing](https://scikit-learn.org/stable/datasets/index.html#boston-dataset) as the dataset, which is already available in Scikit-Learn and was also briefly introduced in the lecture. As always, your training code should be wrapped in a class `LOOCVRidge` that conforms to the standard Scikit-Learn API. **Hint:** It is highly recommended that you standardize the data before performing ridge regression on it.\n",
    "2. One great advantage of a linear model such as ridge regression is that it is very easy to interpret. After training, print the resulting solution $\\mathbf{w}_{\\text{RR}}$ and try to find out how much of an impact each feature has on the result. Now check [the documentation](https://scikit-learn.org/stable/datasets/index.html#boston-dataset) on what each feture represents to see if your results match with your intuition and common sense.\n",
    "3. The root mean squared error is only a very crude metric and does not really tell us much about the behaviour of our regression function. Therefore, we want to visualize the regressor's errors in a bit more detail. In fact, we consider, for each instance $(\\mathbf{x}^{(i)}, y^{(i)})$, the standard squared error $e^2 := (\\mathbf{w}_{\\text{RR}}^T \\mathbf{x}^{(i)}, y^{(i)})^2$ and the non-squared error $e := \\mathbf{w}_{\\text{RR}}^T \\mathbf{x}^{(i)}, y^{(i)}$. Compute both quantities for each instance in the test set and draw a separate boxplot diagram for each. Can you infer something about the behaviour of the classifier from those plots?\n",
    "4. (bonus) If you are in the mood for a challenge, solve parts 1-3 without using a single loop construct (e.g., no `for` loops)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "def loocv(X, y, C):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def get_w_rr(X, y, C):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "class LOOCVRidge(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C_candidates, bias=True):\n",
    "        super(LOOCVRidge, self).__init__()\n",
    "        \n",
    "        self.C_candidates = C_candidates\n",
    "        self.bias = bias\n",
    "        self.w = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError('TODO')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError('TODO')\n",
    "    \n",
    "# Load Boston Housing dataset\n",
    "X, Y = load_boston(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=12345)\n",
    "\n",
    "# TODO: Run LOOCV for a set of possible values for C\n",
    "C_candidates = [10**i for i in range(-3, 3+1)]\n",
    "\n",
    "# TODO: Plot boxplots of errors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
