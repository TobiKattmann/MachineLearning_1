{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning I, Programming Exercise 2\n",
    "\n",
    "## 1. Gradient Descent\n",
    "*Gradient Descent* is an iterative method to find a local minimum of a function $f: \\mathbb{R}^{d} \\to \\mathbb{R}$. The algorithm starts with an arbitrary estimate $\\mathbb{x}^{(0)} \\in \\mathbb{R}^{d}$ and produces a sequence of estimates $\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots$ according to the following update rule:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\alpha_t \\nabla f(\\mathbf{x}^{(t)}),\n",
    "\\end{equation*}\n",
    "where $\\alpha_t \\in \\mathbb{R}_{+}$ is called the *step size* (note that it is allowed to change in every iteration). Since the gradient of $f$ points in the direction of steepest ascent, subtracting the gradient updates the estimate towards the direction of steepest descent. The algorithm terminates when $\\lVert \\nabla f(\\mathbf{x}^{(t)}) \\rVert < \\varepsilon$ for some small $\\varepsilon > 0$, i.e., when it (approximately) reaches a critical point of $f$. Note that convergence is not guaranteed in the general case, so it makes sense to enforce a hard limit on the number of iterations.\n",
    "\n",
    "In this exercise we are going to perform gradient descent on Himmelblau's function, a polynomial of degree four in two variables:\n",
    "\\begin{align}\n",
    "    f: && \\mathbb{R}^{2} &\\to \\mathbb{R}\\\\\n",
    "    &&\\left(\\begin{matrix}x \\\\ y\\end{matrix}\\right) &\\mapsto (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
    "\\end{align}\n",
    "Specifically, your tasks are as follows:\n",
    "1. Implement the gradient descent method on Himmelblau's function. Your routine should return the whole list of iterates $\\mathbf{x}^{(0)}, \\mathbf{x}^{(1)}, \\ldots$, so that you can also visualize the trajectory. The code skeleton already plots Himmelblau's function, so you can use the [`plot`](https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html#line-plots) method of Matplotlib's `Axes3D` object to draw the trajectory into the same figure. Choose $\\mathbf{x}^{(0)}$ uniformly random from the square spanned by the two points $(\\begin{matrix}-6 & -6\\end{matrix})^T$ and $(\\begin{matrix}6 & 6\\end{matrix})^T$ and run the algorithm for several draws of $\\mathbf{x}^{(0)}$.\n",
    "2. Play around with the step size and visualize the result to see what happens.\n",
    "3. Another popular method for finding local optima is called *Newton's Method*. It works exactly in the same way as gradient descent, except that the step size in the update formula is replaced by the inverse of the hessian matrix of $f$:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - H_f(\\mathbf{x}^{(t)})^{-1} \\nabla f(\\mathbf{x}^{(t)})\n",
    "\\end{equation*}\n",
    "Implement Newton's method in the same way as you did for gradient descent including the visualization and run the cell for a few times to see what happens.\n",
    "4. Compare the two methods. What differences did you notice regarding, for example, the number of iterations till convergence, the trajectories, etc.?\n",
    "5. You have probably noticed by now that our simple version of Newton's method often ends up in saddle points or even the local maximum. One way of mitigating this is to perform the update step only if the hessian matrix $H_f(\\mathbf{x}^{(t)})$ is positive definite and to fall back to a simple gradient descent update otherwise. Implement and visualize this 'improved' version of Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def himmelblau_wrapper(func):\n",
    "    \"\"\"\n",
    "    This decorator allows passing one array containing [x,y]\n",
    "    to the functions instead of both parameters separately\n",
    "    \"\"\"\n",
    "    def wrapper(x):\n",
    "        return func(*x)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@himmelblau_wrapper\n",
    "def himmelblau(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "@himmelblau_wrapper\n",
    "def himmelblau_gradient(x, y):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "@himmelblau_wrapper\n",
    "def himmelblau_hessian(x, y):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "\n",
    "def gradient_descent(f, f_gradient, f_hessian, x_0, step_size=0.01, max_iter=1000, eps=1e-7):\n",
    "    # TODO: Optimize f\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def newtons_method(f, f_gradient, f_hessian, x_0, step_size=0.01, max_iter=100, eps=1e-7):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def improved_newton(f, f_gradient, f_hessian, x_0, step_size=0.01, max_iter=100, eps=1e-7):\n",
    "    raise NotImplementedError('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def make_himmelblau_plot(ax, x, y):\n",
    "    ax.view_init(elev=60, azim=-30)\n",
    "    ax.plot_surface(x, y, himmelblau([x,y]), cstride=1, rstride=1, norm=colors.LogNorm(), cmap=plt.cm.jet)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlabel('$z$')\n",
    "\n",
    "x = np.arange(-6, 6, 0.15)\n",
    "y = np.arange(-6, 6, 0.15)\n",
    "x, y = np.meshgrid(x, y)\n",
    "retries = 5\n",
    "x_0 = np.random.uniform(-6, 6, size=(retries, 2))\n",
    "methods = [gradient_descent, newtons_method, improved_newton]\n",
    "\n",
    "fig, axes = plt.subplots(len(methods), 1, figsize=(12, 27), subplot_kw=dict(projection='3d'))\n",
    "for ax, method in zip(axes, methods):\n",
    "    make_himmelblau_plot(ax, x, y)\n",
    "    ax.set_autoscale_on(False)\n",
    "    ax.set_title(method.__name__)\n",
    "    \n",
    "    for t in range(retries):\n",
    "        # TODO: Execute method and plot the trace\n",
    "        pass\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. SVM Implementation\n",
    "It is finally time to get your hands dirty and implement the SVM yourselves! Just for your convenience, here is the unconstrained optimization problem for the linear SVM that we want to train on the dataset $D = \\{(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\}$:\n",
    "\\begin{equation*}\n",
    "    \\min_{\\mathbf{w} \\in \\mathbb{R}^d, b \\in \\mathbb{R}} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + \\frac{C}{n} \\sum_{i=1}^{n} \\max\\left(0, 1 - y^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)} + b)\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "1. Implement the support vector machine using (non-stochastic) gradient descent. Use the general gradient descent function that you have already written in exercise 1 of this sheet. The implementation should also include a flag that determines if a bias parameter $b$ should be learned during training or not.\n",
    "2. Now implement the SVM via stochastic gradient descent as well, i.e., sample a random batch of a given size from the training data for each gradient computation. Using your results from the previous part, this should only require you to change the wrapper function for computing the gradient.\n",
    "3. Train both implementations on the simple two-dimensional toy dataset. Don't allow the SVM to learn a bias, i.e., $b$ should be zero.\n",
    "4. Visualize the SVMs decision boundary and the optimization trajectory using the given code skeleton. This is also a good chance to see if your implementation works properly.\n",
    "5. Once everything is set up, you can start playing around with parameters like step size, batch size, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def hinge_loss(t):\n",
    "    return np.maximum(0, 1 - t)\n",
    "\n",
    "def hinge_gradient(t):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def svm_loss(w, b, C, x, y):\n",
    "    return 0.5*np.sum(w**2, axis=0) + C/x.shape[0] * np.sum(hinge_loss(y*(np.matmul(x, w) + b)), axis=0)\n",
    "\n",
    "def svm_gradient(w, b, C, x, y):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "class SVM_GD(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, bias=True, C=1, step_size=0.01, max_iter=1000):\n",
    "        super(SVM_GD, self).__init__()\n",
    "        \n",
    "        self.bias = bias\n",
    "        self.C = C\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #TODO: call gradient_descent somewhere here\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None or self.b is None:\n",
    "            raise ValueError('Predict can only be called after supplying training data with fit first!')\n",
    "        return np.sign(np.matmul(X, self.w) + self.b)\n",
    "    \n",
    "class SVM_SGD(SVM_GD):\n",
    "    def __init__(self, bias=True, batch_size=32, C=1, step_size=0.01, max_iter=1000):\n",
    "        super(SVM_SGD, self).__init__(bias, C, step_size, max_iter)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #TODO: call gradient_descent somewhere here\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "\n",
    "def apply_line(x, w, b):\n",
    "    a = -w[0]/w[1]\n",
    "    b = -b/w[1]\n",
    "    return a*x+b\n",
    "\n",
    "# Generate toy data\n",
    "n = 200\n",
    "X, Y = make_classification(n, n_features=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, class_sep=1.2, random_state=12345)\n",
    "# Change labels from 0,1 to -1,1\n",
    "Y *= 2; Y -= 1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=12345)\n",
    "\n",
    "svm_gd = SVM_GD(bias=False, step_size=0.1)\n",
    "svm_sgd = SVM_SGD(bias=False, batch_size=32, step_size=lambda t: 1/(t+1))\n",
    "\n",
    "cmap = colors.ListedColormap([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 9), sharex='row', sharey='row')\n",
    "for (clf, name), ax in zip([(svm_gd, 'Gradient Descent'), (svm_sgd, 'Stochastic Gradient Descent')], axes[0]):\n",
    "    # Plot dataset\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap, edgecolor='k')\n",
    "    ax.grid(linestyle='dashed')\n",
    "    ax.set_autoscale_on(False)\n",
    "    x_min, x_max, = ax.get_xlim()\n",
    "    \n",
    "    # TODO: Train classifier and plot decision boundary\n",
    "\n",
    "# TODO: Plot trace\n",
    "# Fill the trace with some dummy data, replace this with real data\n",
    "traces = np.array([[[-0.5, 1.5], [-0.1, 1]], [[-0.13, 1.15], [-0.25, 1.25]]])\n",
    "w_min = np.min(traces, axis=(0,2))\n",
    "w_max = np.max(traces, axis=(0,2))\n",
    "w_1 = np.linspace(w_min[0]-0.2, w_max[0]+0.2, 100)\n",
    "w_2 = np.linspace(w_min[1]-0.2, w_max[1]+0.2, 100)\n",
    "w_1, w_2 = np.meshgrid(w_1, w_2)\n",
    "for clf, trace, ax in zip([svm_gd, svm_sgd], traces, axes[1]):\n",
    "    loss = svm_loss(np.stack([w_1.ravel(), w_2.ravel()], axis=0), 0, clf.C, X_train, Y_train.reshape(-1, 1))\n",
    "    ax.contourf(w_1, w_2, loss.reshape(w_1.shape), levels=10, cmap=plt.cm.jet)\n",
    "    cont = ax.contour(w_1, w_2, loss.reshape(w_1.shape), levels=10, colors='black')\n",
    "    ax.clabel(cont, inline=1)\n",
    "    ax.plot(trace[0], trace[1], marker='x', c='violet')\n",
    "    ax.set_xlabel('$\\mathbf{w}_1$')\n",
    "    ax.set_ylabel('$\\mathbf{w}_2$')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
