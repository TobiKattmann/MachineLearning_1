{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning I, Programming Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label Smoothing\n",
    "In the last programming exercise, we introduced the *cross entropy* loss as an extension of the binary log loss from the lecture. For the output $\\mathbf{z} = f(\\mathbf{x} \\in \\mathbb{R}^k$ of our classifier $f: \\mathcal{X} \\to \\mathbb{R}^k$ and $\\mathbf{x}$'s true label $y \\in \\{1, \\ldots, k\\}$, we defined it like this:\n",
    "\\begin{equation*}\n",
    "    \\ell(\\mathbf{z}, y) = -\\log(\\operatorname{softmax}(\\mathbf{z})_y).\n",
    "\\end{equation*}\n",
    "Note that this value is minimized as $\\operatorname{softmax}(\\mathbf{z})_y \\to 1$. For this, the difference between $\\mathbf{z}_y$ and the other elements of $\\mathbf{z}$ must be very large. The problematic part about this is that the network is encouraged to make predictions with a very high confidence, which can lead to overfitting.\n",
    "\n",
    "However, we can yet again extend the cross entropy loss by using a vector-valued ground-truth label $\\mathbf{y} \\in [0,1]^k$ with $\\sum_{i=1}^{k} y_i = 1$ and the following formulation of the cross entropy loss:\n",
    "\\begin{equation*}\n",
    "    \\ell(\\mathbf{y}, \\mathbf{z}) = -\\sum_{i=1}^{k} y_i \\log\\left(\\operatorname{softmax}\\left(\\mathbf{z}\\right)_i\\right).\n",
    "\\end{equation*}\n",
    "If the true label is $c \\in \\{1, \\ldots, k\\}$, we can recover the previous formulation by setting $y_c = 1$ and $y_i = 0$ for all $i \\in \\{1, \\ldots, k\\} \\setminus \\{c\\}$. This is called *one-hot* encoding of $c$.\n",
    "\n",
    "The idea of *label smoothing* (first proposed in [this paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf) by Szegedy et al.) is to avoid using a one-hot encoded vector as the target label. Instead, one chooses $\\varepsilon \\in [0,1]$ and then builds the vector $\\mathbf{y}$ with\n",
    "\\begin{equation*}\n",
    "    y_c = \\frac{\\varepsilon}{k} + (1-\\varepsilon) \\quad \\text{and} \\quad y_i = \\frac{\\varepsilon}{k} \\text{ for all } i \\in \\{1, \\ldots, k\\} \\setminus \\{c\\}.\n",
    "\\end{equation*}\n",
    "The authors have found that this is an effective and cheap way to prevent the model from becoming too confident and possibly overfitting. Since then, it has become a widely adopted method of regularization in the machine learning research community.\n",
    "\n",
    "1. Unlike other deep learning frameworks, PyTorch does not support label smoothing out of the box. Hence, it is your job to implement label smoothing in a custom module. Writing a custom module, layer or loss works in exactly the same way as writing your own network: subclass `torch.nn.Module` and override the `forward` method. Note that the forward method must take two inputs here: the networks prediction $\\mathbf{z}$ and the ground truth label $c \\in \\{1, \\ldots, k\\}$ in that order. It should also be possible to specify the 'smoothness' parameter $\\varepsilon$ when instantiating the module.\n",
    "\n",
    "   Note that PyTorch generally operates with batches of data insted of single points. This is usually not much of a problem, since it can be handeled transparently, but the output of a loss' `forward` function is expected to be a scalar. Therefore, you need to compute the loss for each data point in the batch separately and the return the average loss over the batch. More precisely, given a batch of data $B = \\{(\\mathbf{x}^{(1)}, c^{(1)}), (\\mathbf{x}^{(2)}, c^{(2)}), \\ldots, (\\mathbf{x}^{(n)}, c^{(\\lvert B \\rvert)})\\} \\subseteq \\mathcal{X} \\times \\{1, \\ldots, k\\}$, you need to convert each label $c^{(i)}$ to its correct vector form $\\mathbf{y}^{(i)} \\in [0, 1]^k$ according to the formulas above and then compute\n",
    "\\begin{equation*}\n",
    "    L(B) = \\frac{1}{B} \\sum_{i=1}^{\\lvert B \\rvert} \\ell(f(\\mathbf{x}^{(i)}), \\mathbf{y}^{(i)}).\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "2. Re-use the code you wrote for parts 2 and 3 of the last programming exercise to train the LeNet-5 network on fashion MNIST once again, but using label smoothing this time. Start with a value of $\\varepsilon=0.3$ and see if that changes the accuracy of the network significantly. If you didn't solve those tasks on the last sheet, you can also copy the code from the sample solution that has been uploaded to Olat.\n",
    "\n",
    "\n",
    "3. After training, we can try and visualize the *confidence* with which the network makes its predictions. Confidence here means the softmax value of the predicted class, or mathematically speaking for input $\\mathbf{x} \\in \\mathcal{X}$ and classifier $f: \\mathcal{X} \\to \\mathbb{R}^k$, we define the confidence as\n",
    "\\begin{equation*}\n",
    "    \\operatorname{conf}(f, \\mathbf{x}) = \\max_{i \\in \\{1, \\ldots, k\\}} \\operatorname{softmax}(f(\\mathbf{x}))_i.\n",
    "\\end{equation*}\n",
    "\n",
    "   We will use several [boxplots](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html) to visualize this quantity by following those steps:\n",
    "    1. Pass the entire validation set to the trained network from part 2 and record the predicted class label and confidence for each input.\n",
    "    2. Split the resulting data into two sets: Correctly classified samples, where ground truth and predction match and another set for those, where this is not the case. The following step should then be applied to each of the sets separately.\n",
    "    3. Partition the data according to the predicted class and draw a boxplot of the confidence values for each partition. Those should be next to each other in the same figure and they should be annotated with the class names they belong to.\n",
    "   \n",
    "   This should allow you see which classes the network predicts with most confidence and it is also interesting to see whether the network is just as confident when it is making a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# ------- old code from previous sheet -----------\n",
    "# TODO: here goes the code from last sheet\n",
    "# you need the LeNet class and the functions train_model and plot_history\n",
    "# ------- old code from previous sheet -----------\n",
    "\n",
    "\n",
    "class SmoothCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(SmoothCrossEntropyLoss, self).__init__()\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "def average_confidence(network, val_iter, label_dict, device='cpu'):\n",
    "    # TODO: Compute and plot the average confidence for classifier 'network' and\n",
    "    # dataset 'val_iter' (given as a DataLoader)\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "\n",
    "# Change this to 'cuda' if you can and want to use a GPU for training\n",
    "# Otherwise, you can completely ignore this parameter\n",
    "device = torch.device('cpu')\n",
    "dataset = torchvision.datasets.FashionMNIST\n",
    "# Those are the classes in fashion MNIST. Their position in the list corresponds to the numerical class label,\n",
    "# i.e., class 0 is 'T-shirt/top'\n",
    "label_dict = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "batch_size = 64\n",
    "num_epochs = 15\n",
    "num_workers = 0  # You can use multiple processes to load the data. 0 means that everything is done in the same process\n",
    "step_size = 1e-2\n",
    "regularization=0.0004  # Contols how much the Frobenius norm of the parameters contributes to the final loss\n",
    "label_smoothing = 0.3  # Controls the amount of label smoothing (epsilon)\n",
    "checkpoints = True  # Save the model after each epoch of training\n",
    "out_dir = 'models'  # Folder, where your model checkpoints will be saved\n",
    "load_path = None  # Don't train the model, but load it from the specified file instead\n",
    "data_dir = 'data'  # Folder that contains the dataset. If it is not present, \n",
    "                   # the dataset will be downloaded into that folder\n",
    "\n",
    "# Load the training and the test dataset and wrap them in a DataLoader\n",
    "is_cuda = device.type == 'cuda'\n",
    "train_dataset = dataset(data_dir, download=True, train=True, transform=torchvision.transforms.ToTensor())\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                    num_workers=num_workers, pin_memory=is_cuda)\n",
    "val_dataset = dataset(data_dir, download=True, train=False, transform=torchvision.transforms.ToTensor())\n",
    "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                    num_workers=num_workers, pin_memory=is_cuda)\n",
    "\n",
    "# TODO: Instantiate the network\n",
    "#activation=lambda x: 1.7159*torch.tanh(x)\n",
    "#network = LeNet(activation=activation, in_channels=1)\n",
    "#network = network.to(device)\n",
    "#loss = SmoothCrossEntropyLoss().to(device)\n",
    "#optimizer = torch.optim.SGD(network.parameters(), lr=step_size, weight_decay=regularization)\n",
    "\n",
    "#start_epoch = 0\n",
    "\n",
    "if load_path is not None:\n",
    "    # Load model, optimizer state and history \n",
    "    #state_dict = torch.load(load_path)\n",
    "    #network.load_state_dict(state_dict['model'])\n",
    "    #optimizer.load_state_dict(state_dict['opt'])\n",
    "    #start_epoch = state_dict['epoch']+1\n",
    "    #history = state_dict['history']\n",
    "    pass\n",
    "else:\n",
    "    # Otherwise, we train the model for the specified number of epochs\n",
    "    #history = train_model(network, loss, optimizer, train_iter, val_iter, num_epochs, device,\n",
    "    #                  start_epoch, checkpoints=checkpoints, out_dir=out_dir)\n",
    "    pass\n",
    "\n",
    "# Plot the training history\n",
    "#plot_history(history)\n",
    "\n",
    "# TODO: Compute average confidence\n",
    "#average_confidence(network, val_iter, label_dict, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization of kernel SVMs\n",
    "\n",
    "In the lecture you have learned that $C$ is a regularization parameter for the SVM. However, sometimes $C$ is not important when we use kernels, because the data became linearly separable in the new high-dimensional space. For many kernel SVMs,  tuning the kernel parameters is often the crucial procedure that determines the generalization strength of the model.\n",
    "\n",
    "In this exercise we will once again have a look at the RBF kernel SVM. It has one parameter, $\\sigma > 0$, and the function looks as follows:\n",
    "\\begin{equation*}\n",
    "    k(\\mathbf{x}, \\mathbf{x}') = \\exp\\left(-\\frac{1}{2\\sigma^2} \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^2\\right).\n",
    "\\end{equation*}\n",
    "$\\sigma$ determines the \"width\" of the RBF kernel, much like the standard deviation $\\sigma$ of a normal distribution.\n",
    "\n",
    "Your tasks will be the following:\n",
    "1. Run the RBF kernel SVM for 25 different values of $\\sigma$ in the range $[10^{-3}, 10^3]$, while fixing $C=1$. Use the [digits dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) like in part 3 of programming exercise 3. You can either use your own SVM implementation from the earlier sheets or the `SVC` class from Scikit-Learn. Modify your `cross_validation` method from ex. 3, s.t. it returns both the training and validation score for each split and use this to obtain scores for the SVM. Choose $k \\geq 5$.\n",
    "2. Make a plot where the x-axis corresponds to the different values of $\\sigma$ and the y-axis is the mean of training and/or validation scores that you obtained before when using the corresponding value for $\\sigma$. Use two different colors for the training and validation curve and annotate which is which.\n",
    "3. In the plot of part 2, determine the underfitting region, the overfitting region and the best values for $\\sigma$.\n",
    "4. (optional) Repeat steps 1-3 using a polynomial kernel instead. Here, the variable on the x-axis should be the degree $d$ that is used in the polynomial kernel\n",
    "\\begin{equation*}\n",
    "    k(\\mathbf{x}, \\mathbf{x}') = \\left(\\gamma \\langle \\mathbf{x}, \\mathbf{x}' \\rangle + c_0\\right)^d.\n",
    "\\end{equation*}\n",
    "Fix $\\gamma=1, c_0=0$ and $C=1$ and try values for $d$ in the range $\\{1, \\ldots, 10\\}$. \n",
    "\n",
    "**Hint:** As mentioned in sheet 3, you need to be careful when using the `SVC` class with the RBF kernel. Instead of $\\sigma$, this class expects a parameter $\\gamma$ and the relation between the two is $\\gamma = 1 / (2\\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def cross_validation(clf, X, Y, k=5):   \n",
    "    # TODO: copy the cross validation method from sheet03 and modify it, so it\n",
    "    # returns the scores on the training set as well\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "X, Y = digits.data, digits.target\n",
    "\n",
    "# TODO: Calculate cross validation accuracies (RBF SVM)\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "sigmas = np.power(10, np.linspace(-3, 3, 25))\n",
    "\n",
    "# TODO: Plot the results\n",
    "\n",
    "# TODO: Calculate cross validation accuracies (Poly SVM)\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "degrees = np.arange(1, 11)\n",
    "\n",
    "# TODO: Plot the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
