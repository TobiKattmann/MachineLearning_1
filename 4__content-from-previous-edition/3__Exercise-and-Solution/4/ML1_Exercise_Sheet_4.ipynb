{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning I, Programming Exercise 4\n",
    "You will probably find that the tasks in this notebook are considerably more difficult than those in the previous exercises. That is because it will examine new concepts that are only touched upon briefly in the lecture. If you get stuck at some point or find that the task descriptions are unclear, please ask your questions in the \"Exercise Questions\" channel of the Mattermost and tag it with `@tmichels` so that I will be notified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch\n",
    "Now that the lecture is starting to focus on neural networks, we will start working with *PyTorch*, as it offers a great variety of features making the handling of neural networks much more convenient. In theory, we could also use Scikit-Learn or even code the NN from scratch in Numpy, but there are a few advantages that make PyTorch a better suited choice for our tasks:\n",
    "* Fast implementations for the most common layer types (fully-connected, convolution, etc.) are available.\n",
    "* GPU support (only for NVIDIA cards, though)\n",
    "* Automatic differentiation (AD) engine that can compute the gradient of (almost) any function automatically\n",
    "\n",
    "Installing the library is again quite simple. Visit [this link](https://pytorch.org/get-started/locally/) and adjust the command in the following cell according to your needs. We will try to keep the computational effort for the exercises in reasonable bounds, so GPU support is not needed but you can install it if you want. Just make sure that the CUDA version you select supports your NVIDIA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Installation using conda and no gpu support\n",
    "!conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch\n",
    "# Alternatively, the command for using pip would look like this:\n",
    "# !{sys.executable} -m pip install torch==1.5.0+cpu torchvision==0.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's low-level API is quite similar to Numpy's, except for fact that the fundamental datatype is called `tensor` instead of `ndarray`. In fact, most functions and attributes (e.g., `dot()`, `shape`, `squeeze()`, etc.) have the same names and functionality. Hence, it should not be too difficult for you to get accustomed to PyTorch.\n",
    "\n",
    "1. Read the first two parts of the [PyTorch Blitz Tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). The first part deals with the basic low-level API, while the second introduces the `Autograd` AD engine. This should give you a rough idea of what PyTorch can and cannot do.\n",
    "2. Remember Himmelblau's function from programming exercise 2? Compute its gradient at several randomly chosen points using only PyTorch's `Autograd` engine. Compare the values with the analytically derived function `himmelblau_gradient` from exercise 2.\n",
    "3. Implement a function `auto_hessian` that computes the Hessian matrix of another arbitrary function in a specified point. It should take the following arguments:\n",
    "    * `func`: A python function that takes a single one-dimensional `tensor` as its argument and outputs a scalar `tensor`. All computatons inside the function should be performed with PyTorch.\n",
    "    * `x`: The point in which to evaluate the Hessian.\n",
    "Test your implementation on Himmelblau's function using the analytically derived `himmelblau_hessian`.\n",
    "\n",
    "   **Hint:** \"The Hessian matrix of a function $f$ is the Jacobian matrix of the gradient of the function: $\\mathbf{H}(f(\\mathbf{x})) = \\mathbf{J}(\\nabla f(\\mathbf{x}))$\" (see [Wikipedia](https://en.wikipedia.org/wiki/Hessian_matrix)). Also take a look at PyTorch's [`torch.autograd.grad`](https://pytorch.org/docs/master/autograd.html#torch.autograd.grad) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def himmelblau_wrapper(func):\n",
    "    \"\"\"\n",
    "    This decorator allows passing one tensor containing [x,y]\n",
    "    to the functions instead of both parameters separately\n",
    "    \"\"\"\n",
    "    def wrapper(x):\n",
    "        return func(*x)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@himmelblau_wrapper\n",
    "def himmelblau(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "@himmelblau_wrapper\n",
    "def himmelblau_gradient(x, y):\n",
    "    x_bar = 4*x*(x**2 + y - 11) + 2*(x + y**2 - 7)\n",
    "    y_bar = 2*(x**2 + y - 11) + 4*y*(x + y**2 - 7)\n",
    "    return torch.tensor([x_bar, y_bar])\n",
    "\n",
    "@himmelblau_wrapper\n",
    "def himmelblau_hessian(x, y):\n",
    "    xx_bar = 4*(x**2 + y - 11) + 8*x**2 + 2\n",
    "    yy_bar = 2 + 4*(x + y**2 - 7) + 8*y**2\n",
    "    xy_bar = 4*x + 4*y\n",
    "    return torch.tensor([[xx_bar, xy_bar], [xy_bar, yy_bar]])\n",
    "\n",
    "\n",
    "def auto_gradient(func, x):\n",
    "    assert x.ndim == 1\n",
    "    \n",
    "    # TODO: use autograd to compute the gradient of func(x) w.r.t. x and return it\n",
    "    return x\n",
    "\n",
    "def auto_hessian(func, x):\n",
    "    assert x.ndim == 1\n",
    "    \n",
    "    # TODO: use autograd to compute the hessian of func(x) w.r.t. x and return it\n",
    "    return torch.matmul(x.reshape(-1, 1), x.reshape(1, -1))\n",
    "\n",
    "\n",
    "x = torch.rand(2)\n",
    "\n",
    "g_auto = auto_gradient(himmelblau, x)\n",
    "print('Gradient calculated by Autograd:')\n",
    "print(g_auto)\n",
    "g_hand = himmelblau_gradient(x)\n",
    "print('Gradient calculated by hand:')\n",
    "print(g_hand)\n",
    "assert torch.all(g_auto == g_hand)\n",
    "\n",
    "h_auto = auto_hessian(himmelblau, x)\n",
    "print('Hessian calculated by Autograd:')\n",
    "print(h_auto)\n",
    "h_hand = himmelblau_hessian(x)\n",
    "print('Hessian calculated by hand:')\n",
    "print(h_hand)\n",
    "assert torch.all(h_auto == h_hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolution\n",
    "The *convolution* operation is of central importance for many modern neural network architectures, especially when it comes to processing images. Before we delve into details, it is a good idea to gain some insights into the fundamentals.\n",
    "\n",
    "Mathematically speaking, we define the convolution of two *discrete* functions $f, g: \\mathbb{Z} \\to \\mathbb{R}$ in the following way:\n",
    "\\begin{equation*}\n",
    "    (f * g)(x) = \\sum_{k=-\\infty}^{\\infty} f(k)g(x-k)\n",
    "\\end{equation*}\n",
    "For the definiton to make sense, the infinite sum should converge, otherwise the convolution of $f$ and $g$ is not defined in $x$. Note that this definiton also works if $f$ and $g$ are finite sequences: We can simply set all other values to 0, which makes the summation above finite and therefore computable. The operation itself also has a number of useful algebraic properties, which can be easily proven using the definition above:\n",
    "* *Commutativity*: $(f*g) = (g*f)$, which means that we can switch filter and signal.\n",
    "* *Associativity*: $(f*g)*h = f*(g*h)$\n",
    "* *Distributivity*: $f*(g+h) = (f*g) + (f*h)$\n",
    "* *Scalar multiplication*: $(\\lambda f) * g = \\lambda (f * g)$ for $\\lambda \\in \\mathbb{R}$\n",
    "\n",
    "We can extend the definition to multiple input dimensions, i.e., functions $f, g: \\mathbb{Z}^n \\to \\mathbb{R}$ in a straightforward manner:\n",
    "\\begin{equation*}\n",
    "    (f*g)(x_1, \\ldots, x_n) = \\sum_{k_1, \\ldots, k_n=-\\infty}^{\\infty} f(k_1, \\ldots, k_n)g(x_1-k_1, \\ldots, x_n-k_n)\n",
    "\\end{equation*}\n",
    "\n",
    "But for now, we'll stick with the one-dimensional case to keep things as simple as possible. Consider the sequences `f=[1, 2]` and `g=[8, 3, 19, 13, 12, 14]`, which we can embed into functions $f, g: \\mathbb{Z} \\to \\mathbb{R}$ as follows:\n",
    "\\begin{equation*}\n",
    "    f(0)=1, f(1)=2 \\text{ and } f(z)=0 \\text{ for all } z \\in \\mathbb{Z} \\setminus \\{0,1\\}\n",
    "\\end{equation*}\n",
    "and\n",
    "\\begin{equation*}\n",
    "    g(0)=8, g(1)=3, g(2)=19, g(3)=13, g(4)=12, g(5)=14 \\text{ and } g(z)=0 \\text{ for all } z \\in \\mathbb{Z} \\setminus \\{0,\\ldots,5\\}.\n",
    "\\end{equation*}\n",
    "Now compute and note, on a sheet of paper or in your head, the convolution $f*g$ according to the definiton above. Since both $f$ and $g$ have finite support, we should also be able to compute this result on a computer. In fact, PyTorch offers the function [`torch.nn.functional.conv1d`](https://pytorch.org/docs/stable/nn.functional.html#conv1d) for cases like this. Don't let the documentation confuse you for now, we will learn what each of the parameters do later in the exercise. Let's just throw the function at our example instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "f = torch.tensor([1, 2]).reshape(1, 1, -1)\n",
    "g = torch.tensor([8, 3, 19, 13, 12, 14]).reshape(1, 1, -1)\n",
    "res = torch.nn.functional.conv1d(g, f)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is definitely not what we would have expected! Upon closer inspection of the result, we can deduce that PyTorch has computed something like a sliding dot product, where the *filter* $f$ is sliding along the *signal* $g$, but that does *not* correspond to the definition of a convolution. In fact, the name of the PyTorch function is quite misleading, as it does not compute the convolution of $f$ and $g$, but another operation called *cross-correlation* instead. For discrete functions $f, g: \\mathbb{Z} \\to \\mathbb{R}$ this operation is defined as\n",
    "\\begin{equation*}\n",
    "    (f \\star g)(x) = \\sum_{k=-\\infty}^{\\infty} f(k)g(x+k)\n",
    "\\end{equation*}\n",
    "and from this definiton it is immediately clear that $f*g = \\tilde{f} \\star g$ with $\\tilde{f}(x) = f(-x)$. In other words, we have to flip the filter tensor before we pass it to the `conv1d` function.\n",
    "\n",
    "However, there is another discrepancy between theory and practice in the way how boundary cases are handeled. In order to embed the finite sequences into functions that are defined over all of $\\mathbb{Z}$, we added an infinite amount of zeros to both sides of the sequence, but the PyTorch implementation does not consider such a step by default. Instead, we have to manually pad the sequence using the `padding` parameter of the `conv1d` function. Setting `padding=m` means that $m$ zeros are added to both sides of the given input tensor (`g` in our example). If our filter $f$ is finite and has size $l$, we need to pad with least $l-1$ zeros to achieve the result that we would expect from the mathematical definition.\n",
    "\n",
    "If we make said changes to the code above, we finally get the expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_rev = torch.flip(f, dims=[-1])\n",
    "res = torch.nn.functional.conv1d(g, f_rev, padding=1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basic understanding of what the \"convolution\" functions in PyTorch actually do, we can switch our focus to images and hence we will use [`conv2d`](https://pytorch.org/docs/stable/nn.functional.html#conv2d) from now on. In order to understand what the parameters of ths function mean, please read the documentation carefully and also take a look at [this link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md). It shows a collection of animated figures, each visualizing a different setting for the parameters. \n",
    "\n",
    "1. You have probably noticed that applying a convolution filter to an input changes the spatial size of the output. Take a close look at the documentation for the [`Convolution2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) layer to understand how the paramters influence the output size. Often, one does not want to change the spatial size during the convolution, which means that padding must be used. Your job is to write a function that calculates the required amount of padding, s.t. the output of a convoluton has the same size as the original input. More specifically, it should accept the following arguments:\n",
    "    * `kernel_size`: An integer or 2-tuple (height, width) that specifies the spatial dimensions of the filter (also called kernel).\n",
    "    * `input_size`: An integer or 2-tuple (height, width) that specifies the spatial dimensions of the input image.\n",
    "    * `stride`: An integer or 2-tuple (vertical, horizontal) that specifies the striding of the convolution operation.\n",
    "    * `dilation`: An integer or 2-tuple (vertical, horizontal) that specifies the dilation rate for the filter.\n",
    "    \n",
    "   Finally, return a 4-tuple `(pad_left, pad_right, pad_top, pad_bottom)` that specifies the required amount of padding on each side of the image. It should be possible to pass those values to the [`pad`](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.pad) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calc_same_pad2d(in_shape, kernel_size, stride=1, dilation=1):\n",
    "    # TODO: compute the correct padding and return it\n",
    "    pad_left = pad_right = pad_top = pad_bottom = 0\n",
    "    return pad_left, pad_right, pad_top, pad_bottom\n",
    "\n",
    "\n",
    "kernel_size = tuple(torch.randint(1, 6, size=(2,)).numpy())\n",
    "image_size = tuple(torch.randint(20, 256, size=(2,)).numpy())\n",
    "stride = torch.randint(1, 4, size=(1,)).item()\n",
    "dilation = torch.randint(1, 3, size=(1,)).item()\n",
    "\n",
    "# PyTorch's image format is (B, C, H, W), where C is the number of color channels (usually 3)\n",
    "# and H, W are the height and width of the image, respectively.\n",
    "# B is called the batch_size and it is simply an extra dimension that allows us to concatenate\n",
    "# several images and process them in parallel.\n",
    "image = torch.rand((1,3)+image_size)\n",
    "# The kernel tensor has shape (K, C', KH, HW), where K is the number of filters that should\n",
    "# be processed in parallel, C' is the number of input channels (usually C'=C) and\n",
    "# KH, KW are the height and width of the filter, respectively.\n",
    "kernel = torch.rand((1, 3)+kernel_size)\n",
    "\n",
    "print('Parameters:')\n",
    "print('Image size:', image_size)\n",
    "print('Kernel size:', kernel_size)\n",
    "print('Stride:', stride)\n",
    "print('Dilation:', dilation)\n",
    "\n",
    "padding = calc_same_pad2d(image_size, kernel_size, stride, dilation)\n",
    "print('Calculated Padding:', padding)\n",
    "pad_img = torch.nn.functional.pad(image, padding, mode='constant')\n",
    "print('Padded image size:', pad_img.shape[-2:])\n",
    "\n",
    "res_img = torch.nn.functional.conv2d(pad_img, kernel, stride=stride, dilation=dilation)\n",
    "print('Conv. output size:', res_img.shape[-2:])\n",
    "assert image_size == res_img.shape[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the lecture, you have heard about *pooling* operations. Generally speaking, they subdivide the image into non-overlapping patches of the same size and apply an aggregation function on each patch separately. It turns out that *average pooling*, i.e., the aggregation function is simply the average of each patch, can be represented by a convolution. Your task is to implement avg. pooling with an arbitrary patch size of $(h, w)$ using the `conv2d` function. That means you'll have to come up with the correct set of parameters that need to be passed to the function. Compare your results with what is output by the [`avg_pool2d`](https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def my_avg_pool2d(input, kernel_size, padding=0):\n",
    "    # TODO: compute the average pooling operator using the\n",
    "    # torch.nn.functional.conv2d function instead\n",
    "    return torch.nn.functional.max_pool2d(input, kernel_size,padding=padding)\n",
    "\n",
    "\n",
    "img = torch.randint(10, size=(3, 8, 8)).to(torch.get_default_dtype()).unsqueeze(0)\n",
    "print('The orginal image:')\n",
    "print(img)\n",
    "my_pool_img = my_avg_pool2d(img, 2)\n",
    "print('Custom avg. pooling result:')\n",
    "print(my_pool_img)\n",
    "pool_img = torch.nn.functional.avg_pool2d(img, 2)\n",
    "print('PyTorch avg. pooling result:')\n",
    "print(pool_img)\n",
    "assert torch.all(my_pool_img == pool_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. One of the most widely used filters in digital signal processing and also image processing is the Gaussian blur. The name comes from the fact that it approximates the probability density function of Gaussian random variable with mean $0$ and variance $\\sigma^2 > 0$:\n",
    "\\begin{equation*}\n",
    "    f_{\\sigma}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)\n",
    "\\end{equation*}\n",
    "Strictly speaking, the function is positive everywhere, which would mean that a discrete filter representing it must be infinitely large. To keep things feasible, all values that have a distance larger than $c\\sigma$ ($c>0$, usually $c=3$) to the center at $0$ are usually truncated and set to $0$. That means the size of the discrete filter is determined by $\\sigma$ and it will look like this:\n",
    "\\begin{equation*}\n",
    "    \\left[f_{\\sigma}(-\\lceil c\\sigma \\rceil), \\ldots, f_{\\sigma}(-1), f_{\\sigma}(0), f_{\\sigma}(1), \\ldots, f_{\\sigma}(\\lceil c\\sigma \\rceil)\\right]\n",
    "\\end{equation*}\n",
    "In general, the elements of this sequence will not add up to 1, but this can be fixed by simply dividing them by their sum.\n",
    "\n",
    "   Since we are working with images and therefore two-dimensional signals, we might want to consider the case of a Gaussian random variable with mean $\\mathbf{0}$ and covariance matrix $\\sigma^2 I$ instead:\n",
    "\\begin{equation*}\n",
    "    f(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)\n",
    "\\end{equation*}\n",
    "The discrete approximation works analogously to the one-dimensional case.\n",
    "\n",
    "   Consider the following tasks:\n",
    "    * Implement a function that takes $\\sigma$ and $c$ as arguments and returns the discrete approximation to the one-dimensional Gaussian probability density function as a PyTorch tensor. Make sure that the elements of the returned `tensor` sum up to 1.\n",
    "    * Repeat the previous assignment for the 2D case and apply the resulting filter to the test image given in the code skeleton for a few values of $\\sigma$. Use your function from part 1 to ensure that the output image has the same size as the input.\n",
    "    * Let $\\mathbf{g} \\in \\mathbb{R}^d$ be an approximation to a Gauss filter with variance $\\sigma^2$. We can also interpret $\\mathbf{g}$ as an image $I_g$ of size $d \\times 1$ and at the same time as a 2D filter $f_g$ of size $1 \\times d$. Compute the convolution $f_g * I_g$ with appropriate zero padding ($d-1$ horizontally on both sides) and compare the result to the 2D filter obtained in the previous part.\n",
    "    * Given the results from the last part and recalling the algebraic properties of the convolution operation, can you think of a way to speed up the 2D Gaussian blur for larger kernel sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def gaussian_filter1d(sigma, cutoff=3):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def gaussian_filter2d(sigma, cutoff=3):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def gaussian_blur_slow(input, sigma):\n",
    "    assert input.ndim == 4\n",
    "    \n",
    "    # TODO: implement Gaussian blur using the 2D filter\n",
    "    return input\n",
    "\n",
    "def gaussian_blur_fast(input, sigma):\n",
    "    assert input.ndim == 4\n",
    "    \n",
    "    # TODO: implement Gaussian blur using the trick you came up with\n",
    "    return gaussian_blur_slow(input, sigma)\n",
    "\n",
    "\n",
    "def plot_image(ax, image):\n",
    "    image = image.squeeze(0)\n",
    "    image = to_pil_image(image)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image)\n",
    "\n",
    "# Download a test image\n",
    "with urlopen('https://homepages.cae.wisc.edu/~ece533/images/cat.png') as f:\n",
    "    test_img = Image.open(f)\n",
    "test_img = to_tensor(test_img)\n",
    "test_img = test_img.unsqueeze(0)\n",
    "\n",
    "# Apply the gaussian blur function\n",
    "sigma = 2\n",
    "start = time.perf_counter()\n",
    "test_img_blur = gaussian_blur_slow(test_img, sigma)\n",
    "print(f'Slow function took {time.perf_counter()-start:.4f} seconds.')\n",
    "start = time.perf_counter()\n",
    "test_img_blur2 = gaussian_blur_fast(test_img, sigma)\n",
    "print(f'Fast function took {time.perf_counter()-start:.4f} seconds.')\n",
    "assert test_img.shape == test_img_blur.shape\n",
    "assert test_img.shape == test_img_blur2.shape\n",
    "assert torch.allclose(test_img_blur, test_img_blur2)\n",
    "\n",
    "images = [\n",
    "    ('Org. Image', test_img), \n",
    "    ('2D Gaussian Blur Filter', test_img_blur), \n",
    "    ('Faster Gaussian Blur', test_img_blur2)\n",
    "]\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(12, 4*len(images)))\n",
    "for ax, (name, img) in zip(axes, images):\n",
    "    ax.set_title(name)\n",
    "    plot_image(ax, img)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (**optional**) Convolutional Neural Networks (CNNs) make use of the fact that filters don't need to be fixed and can be learned during training. But what kind of filters does a neural network come up with? In this exercise we will try to find out exactly that by examining filters of an already trained network. Luckily, the `torchvision` package, which should have been installed alongside PyTorch, gives us access to a number of CNNs that have been trained on the [ImageNet](http://image-net.org/challenges/LSVRC/2012/index) dataset containing 1.2 million training images with 1000 different classes. Take a look at [this link](https://pytorch.org/docs/stable/torchvision/models.html#classification) for more information on how to download and use the models.\n",
    "\n",
    "   Your tasks are now as follows:\n",
    "    * Create an instance of the pretrained `resnet18` model and extract all filters from the first convolution layer called `conv1`. Remember that this will be a single tensor of shape `(K, C', KH, KW)` as explained in part 1. Use the given function to visualize all `K` filters separately as color images.\n",
    "    * Preprocess the test image that is downloaded by the code skeleton according to the documentation [here](https://pytorch.org/docs/stable/torchvision/models.html#classification). \n",
    "    * Each layer in a PyTorch network allows us to register a hook that can capture input and output generated by that specific layer (see [`torch.nn.Module.register_forward_hook`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_forward_hook)). We want to use this to capture the output of the first conv. layer after we pass our test image to the network. Since each filter produces a single output image with 1 channel, we can visualize them either as grey-scale images or as a heatmap. Take a look at [the matplotlib documentation](https://matplotlib.org/tutorials/colors/colormaps.html) to find a colormap that suits your taste.\n",
    "\n",
    "**Hint:** We can pass a batch of images as a tensor `images` of shape `(B, C, H, W)` to a PyTorch model by calling the model with the image tensor: `output = model(images)`. The actual output (i.e., the prediction) of the network is not very interesting to us, since we just want to know about the intermediate result after the first conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image, normalize\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def preprocess_image(image):\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def plot_image(ax, image, *args, **kwargs):\n",
    "    image = image.squeeze(0)\n",
    "    image = to_pil_image(image)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, *args, **kwargs)\n",
    "\n",
    "\n",
    "# Download test image\n",
    "with urlopen('https://seafile.rlp.net/f/c0ff8ca53b5e475aa89b/?dl=1') as f:\n",
    "    test_img = Image.open(f)\n",
    "# TODO: Preprocess the test image\n",
    "\n",
    "\n",
    "# TODO: Download the model and extract all filters from the first conv. layer\n",
    "model = resnet18(pretrained=True).eval()\n",
    "\n",
    "# TODO: Add a hook to the model that captures the output of the first conv. layer\n",
    "\n",
    "\n",
    "# TODO: Pass the test image through the model and extract the conv. layer output\n",
    "\n",
    "\n",
    "# Plot test image\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "plt.title('The test image')\n",
    "plt.axis('off')\n",
    "plt.imshow(test_img)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# TODO: Plot filters\n",
    "\n",
    "\n",
    "# TODO: Plot conv. output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
